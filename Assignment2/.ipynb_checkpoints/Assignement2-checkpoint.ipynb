{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 \n",
    "- 1D 2 class Gaussian Discriminent\n",
    "- 2D 2 class Gaussian Discriminent\n",
    "- kD k class Gaussian Discriminent\n",
    "### setting all the imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all imports\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.cross_validation import KFold    \n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from numpy.linalg import inv\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import collections # this is used to perform the collection  based sorting  and unique element identification\n",
    "from sklearn.cross_validation import train_test_split #  this is used for cross validation \n",
    "from functools import partial # this is to accomdate partials\n",
    "import random\n",
    "from numpy.linalg import det\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This a class tha twould be used to test the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TestMetrics:\n",
    "    \n",
    "    def setData(self,y_test,y_pred_class):\n",
    "            self.y_test = y_test\n",
    "            self.y_pred_class = y_pred_class\n",
    "            print 'True:', y_test\n",
    "            print 'Pred:', y_pred_class\n",
    "        \n",
    "    def getMatrics(self):\n",
    "        #print metrics.accuracy_score(self.y_test, self.y_pred_class)\n",
    "        # save confusion matrix and slice into four pieces\n",
    "        confusion = metrics.confusion_matrix(self.y_test, self.y_pred_class)\n",
    "#         print \"confusion matrix:\",confusion\n",
    "        TP = confusion[1, 1]\n",
    "        TN = confusion[0, 0]\n",
    "        FP = confusion[0, 1]\n",
    "        FN = confusion[1, 0]\n",
    "        if math.isnan(TP):\n",
    "            print \"tp is nan\"\n",
    "            TP = 0;\n",
    "        if math.isnan(TN):\n",
    "            print \"tn is nan\"\n",
    "            TN = 0;\n",
    "        if math.isnan(FP):\n",
    "            print \"fp is nan\"\n",
    "            FP = 0;\n",
    "        if math.isnan(FN):\n",
    "            print \" is nan\"\n",
    "            FN = 0;            \n",
    "            \n",
    "            \n",
    "        #Classification Accuracy: Overall, how often is the classifier correct?\n",
    "        #Classification_Accuracy = (TP + TN) / float(TP + TN + FP + FN)\n",
    "        Classification_Accuracy = metrics.accuracy_score(self.y_test, self.y_pred_class)\n",
    "        #print \"#Classification Accuracy:\\t\",Classification_Accuracy\n",
    "       \n",
    "        #Classification Error: Overall, how often is the classifier incorrect?\n",
    "        #Classification_Error = (FP + FN) / float(TP + TN + FP + FN)\n",
    "        #print \"#Classification Error:\\t\",Classification_Error\n",
    "        Classification_Error =  1 - metrics.accuracy_score(self.y_test, self.y_pred_class)\n",
    "        \n",
    "        #Sensitivity:\n",
    "        Sensitivity = TP / float(TP + FN)\n",
    "        #print \"#Sensitivity:\\t\",Sensitivity\n",
    "        #print metrics.recall_score(y_test, y_pred_class)\n",
    "        #Specificity: \n",
    "        Specificity = TN / float(TN + FP) \n",
    "        #print \"#Specificity:\\t\",Specificity\n",
    "        #False Positive Rate:\n",
    "        False_Positive_Rate = FP / float(TN + FP)\n",
    "        #print \"#False Positive Rate:\",False_Positive_Rate\n",
    "        #Precision: \n",
    "        #Precision = TP / float(TP + FP)\n",
    "        Precision = metrics.precision_score(self.y_test, self.y_pred_class)\n",
    "        #print \"#Precision:\\t\",Precision\n",
    "        #print metrics.precision_score(y_test, y_pred_class)\n",
    "        #print TP,TN,FP,FN,Classification_Accuracy,Classification_Error,Sensitivity,Specificity,False_Positive_Rate,Precision\n",
    "        return TP,TN,FP,FN,Classification_Accuracy,Classification_Error,Sensitivity,Specificity,False_Positive_Rate,Precision\n",
    "#test        \n",
    "# tm  = TestMetrics()\n",
    "# tm.setData(Y,Y)\n",
    "# tm.getMatrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "# fetching all data\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:,:]  # we only take the first two features.\n",
    "Y = iris.target\n",
    "print Y\n",
    "numberOfClass = lambda listValues :list(set(listValues))# this is the number of classes in each dataset\n",
    "\n",
    "m = lambda listValues: collections.Counter(listValues)\n",
    "\n",
    "idf = lambda currentClass,itratingClass : (0,1)[currentClass == itratingClass] # this is the indicator function\n",
    "\n",
    "mean = lambda X_D,Y_D:[np.mean([idf(Y_D[rowIndex],classElement)*rowElements for rowIndex,rowElements in enumerate(X_D)if idf(Y_D[rowIndex],classElement)== 1],axis = 0) for classIndex ,classElement in enumerate(numberOfClass(Y_D))]\n",
    "\n",
    "varience =lambda X_D,Y_D:[np.var([idf(Y_D[rowIndex],classElement)*rowElements for rowIndex,rowElements in enumerate(X_D)if idf(Y_D[rowIndex],classElement)== 1],axis = 0) for classIndex ,classElement in enumerate(numberOfClass(Y_D))]\n",
    "\n",
    "alpha = lambda X_D,Y_D:[m(Y_1D)[classElement]*1.0/len(Y_D) for classElement in (numberOfClass(Y_D))]\n",
    "# this is for 1D calculation\n",
    "g = lambda x,var,mean,alpha: np.log2(alpha)- (((x-mean)**2)/2*(var**2))\n",
    "\n",
    "covVar =  lambda X_D,Y_D:[np.cov(np.asarray([list(idf(Y_D[rowIndex],classElement)*rowElements) for rowIndex,rowElements in enumerate(X_D)if idf(Y_D[rowIndex],classElement)== 1]).T) for classIndex,classElement in enumerate(numberOfClass(Y_D))]\n",
    "# this is for 2D calcultaion\n",
    "#g_multiVariate = lambda x,co_var_mat,mean: ((2.0*(np.dot(np.dot(x.T,inv(co_var_mat)),mean))) - float(np.dot(np.dot(mean.T,inv(co_var_mat)),mean)))\n",
    "g_multiVariate = lambda x,co_var_mat,mean,aph: (-1.0*np.log((2*np.pi)**x.shape[1])) - (1.0*np.log(det(co_var_mat)))-(0.5*np.dot(np.dot((x-mean),inv(co_var_mat)),(x-mean).T)) +log(aph)\n",
    "\n",
    "# discriminent = lambda x,a_class,b_class : (1,0)[ a_class(x) > b_class(x) \n",
    "# discriminent = lambda x,a_class,b_class: 1 if  a_class(x)>b_class(x) else 0                                           \n",
    "def discriminent (x, a_class, b_class):\n",
    "    sendX = (np.asarray([x])).T # sending x as a collection of rows vector\n",
    "    if (a_class(sendX)>b_class(sendX)) :\n",
    "        return 0\n",
    "    else :\n",
    "        return 1\n",
    "\n",
    "\n",
    "def discriminent_1 (x, a_class, b_class,c_class):\n",
    "    sendX = (np.array([x])) # sending x as a collection of rows vector\n",
    "#     print \"val\",a_class(sendX)\n",
    "#     print \"val2\",b_class(sendX)\n",
    "#     print \"val3\",c_class(sendX)\n",
    "    \n",
    "    if ((a_class(sendX) > b_class(sendX)))and ((a_class(sendX)>c_class(sendX))):\n",
    "        return 0\n",
    "    elif((b_class(sendX)>c_class(sendX))>0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "# def discriminent_1 (x, a_class, b_class,c_class):\n",
    "#     sendX = (np.asarray([x])).T # sending x as a collection of rows vector\n",
    "# #     print \"val\",a_class(sendX)\n",
    "# #     print \"val2\",b_class(sendX)\n",
    "# #     print \"val3\",c_class(sendX)\n",
    "    \n",
    "#     if ((a_class(sendX) > b_class(sendX)))and ((a_class(sendX)>c_class(sendX))):\n",
    "#         return 0\n",
    "#     elif((b_class(sendX)>c_class(sendX))>0):\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 2\n",
    "# test \n",
    "#covVar(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GDA1D2C:\n",
    "    #this si to fit the existing data\n",
    "    def fit(self,Xdata,Ydata):\n",
    "        self.XData = Xdata\n",
    "        self.YData = Ydata\n",
    "        #step:1 - find alpha\n",
    "        self.alpha = alpha(Xdata,Ydata)\n",
    "        \n",
    "#         print \"alpha\"\n",
    "#         print self.alpha\n",
    "        #step:2 - find mean \n",
    "        self.mean = mean(Xdata,Ydata)\n",
    "        print \"mean\",self.mean\n",
    "        #step:3 - find varience\n",
    "        self.varience = varience(Xdata,Ydata)\n",
    "        print\"varience\",self.varience\n",
    "        #step:4\n",
    "        g0 = partial(g,var = self.varience[0],mean= self.mean[0],alpha= self.alpha[0])\n",
    "        g1 = partial(g,var = self.varience[1],mean= self.mean[1],alpha= self.alpha[1])\n",
    "        #step:5\n",
    "        self.discriminent = partial(discriminent,a_class = g0, b_class = g1)\n",
    "        #self.discriminent = partial(discriminent,a_class = partial(g,varience = self.varience[0],mean= self.mean[0],alpha= self.alpha[0]), b_class = partial(g,varience = self.varience[1],mean= self.mean[1],alpha= self.alpha[1]))\n",
    "    # this is to predict y data  from the give X data\n",
    "    def predict(self,XD):\n",
    "        return [self.discriminent(xvalue) for xvalue in XD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GDAnD2C:\n",
    "    #this si to fit the existing data\n",
    "    def fit(self,Xdata,Ydata):\n",
    "        self.XData = Xdata\n",
    "        self.YData = Ydata\n",
    "    \n",
    "        #step:1 - find alpha\n",
    "        self.alpha = alpha(Xdata,Ydata)\n",
    "        #step:2 - find mean \n",
    "        self.mean = mean(Xdata,Ydata)\n",
    "        #print \"mean\",self.mean\n",
    "        \n",
    "        #step:3 - find co_varience\n",
    "        self.co_varience = covVar(Xdata,Ydata)\n",
    "        #print\"co varience matix\",self.co_varience\n",
    "        \n",
    "        #step:4\n",
    "        #print \"mean--------\", ((np.asarray([self.mean[0]])).T).shape\n",
    "        \n",
    "        ## sending the mean as transposed\n",
    "        self.g0 = partial(g_multiVariate,aph = self.alpha[0],co_var_mat = self.co_varience[0],mean= (np.asarray([self.mean[0]])))\n",
    "        self.g1 = partial(g_multiVariate,aph = self.alpha[1],co_var_mat = self.co_varience[1],mean= (np.asarray([self.mean[1]])))\n",
    "        #step:5\n",
    "        self.discriminent = partial(discriminent,a_class = self.g0, b_class = self.g1)\n",
    "        #self.discriminent = partial(discriminent,a_class = partial(g,varience = self.varience[0],mean= self.mean[0],alpha= self.alpha[0]), b_class = partial(g,varience = self.varience[1],mean= self.mean[1],alpha= self.alpha[1]))\n",
    "    # this is to predict y data  from the give X data\n",
    "    def predict(self,XD):\n",
    "        return [self.discriminent(xvalue) for xvalue in XD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean [array([ 4.97142857]), array([ 5.93333333])]\n",
      "varience [array([ 0.13394558]), array([ 0.26513889])]\n",
      "True: [0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Pred: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "mean [array([ 5.02888889]), array([ 5.98222222])]\n",
      "varience [array([ 0.12516543]), array([ 0.24057284])]\n",
      "True: [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "Pred: [0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "mean [array([ 5.0326087]), array([ 5.925])]\n",
      "varience [array([ 0.11611059]), array([ 0.28551136])]\n",
      "True: [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
      "Pred: [0, 0, 0, 0, 1, 1, 1, 1, 1, 0]\n",
      "mean [array([ 5.00666667]), array([ 5.90666667])]\n",
      "varience [array([ 0.12951111]), array([ 0.24906667])]\n",
      "True: [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "Pred: [0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
      "mean [array([ 4.99772727]), array([ 5.92391304])]\n",
      "varience [array([ 0.11158574]), array([ 0.26094991])]\n",
      "True: [0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
      "Pred: [0, 0, 1, 0, 0, 0, 0, 1, 1, 1]\n",
      "mean [array([ 5.00851064]), array([ 5.96976744])]\n",
      "varience [array([ 0.12843821]), array([ 0.25466739])]\n",
      "True: [0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "Pred: [0, 0, 0, 0, 1, 1, 1, 1, 0, 0]\n",
      "mean [array([ 5.00454545]), array([ 5.94130435])]\n",
      "varience [array([ 0.1186157]), array([ 0.26720699])]\n",
      "True: [0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
      "Pred: [0, 0, 0, 0, 0, 0, 1, 1, 0, 0]\n",
      "mean [array([ 5.00681818]), array([ 5.94347826])]\n",
      "varience [array([ 0.11381715]), array([ 0.25332703])]\n",
      "True: [0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
      "Pred: [0, 0, 0, 1, 0, 0, 1, 1, 0, 1]\n",
      "mean [array([ 4.9893617]), array([ 5.88837209])]\n",
      "varience [array([ 0.118823]), array([ 0.23730665])]\n",
      "True: [0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "Pred: [0, 1, 0, 0, 1, 1, 0, 1, 0, 1]\n",
      "mean [array([ 5.01086957]), array([ 5.94545455])]\n",
      "varience [array([ 0.11922968]), array([ 0.29020661])]\n",
      "True: [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
      "Pred: [0, 0, 0, 0, 1, 1, 1, 1, 0, 1]\n",
      "   Accuracy  Error  FN  FP  False_Positive_Rate  Precision  Sensitivity  \\\n",
      "0       0.9    0.1   1   0             0.000000       1.00     0.500000   \n",
      "1       0.7    0.3   3   0             0.000000       1.00     0.400000   \n",
      "2       0.9    0.1   1   0             0.000000       1.00     0.833333   \n",
      "3       0.9    0.1   1   0             0.000000       1.00     0.800000   \n",
      "4       0.8    0.2   1   1             0.166667       0.75     0.750000   \n",
      "\n",
      "   Specificity  TN  TP  \n",
      "0     1.000000   8   1  \n",
      "1     1.000000   5   2  \n",
      "2     1.000000   4   5  \n",
      "3     1.000000   5   4  \n",
      "4     0.833333   5   3  \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkoAAAJPCAYAAACD7vsqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJCIiIlKBipKIiIhIBT8Ahugik5jeJu8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d5c78d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_1D = X[:100,:1]# this is to take one feature and first 100 data sets\n",
    "Y_1D = Y [:100] # this is to take 1st 100 data sets\n",
    "\n",
    "\n",
    "matricsTotal = []\n",
    "kf = sklearn.cross_validation.KFold(n=len(Y_1D), n_folds=10, shuffle=True,random_state=5)\n",
    "for train_index, test_index in kf:\n",
    "    #print train_index\n",
    "#     X_train, X_test, Y_train, Y_test = train_test_split(X_1D, Y_1D, test_size=0.4, random_state= a)\n",
    "    X_train, X_test =[list(X_1D[a]) for a  in train_index],[list(X_1D[a]) for a  in test_index]\n",
    "    Y_train, Y_test = [Y_1D[a] for a  in train_index],[Y_1D[a] for a  in test_index]\n",
    "#     print X_train\n",
    "#     #     print Y_train\n",
    "#     print \"\\n\\n\"\n",
    "    grad =  GDA1D2C()\n",
    "    grad.fit(X_train,Y_train)\n",
    "    predicted_Y_values = grad.predict(X_test)\n",
    "    tm  = TestMetrics()\n",
    "    tm.setData(Y_test,predicted_Y_values)\n",
    "    TP,TN,FP,FN,Classification_Accuracy,Classification_Error,Sensitivity,Specificity,False_Positive_Rate,Precision =  tm.getMatrics()\n",
    "    matricsTotal.append({\"TP\":TP,\"TN\":TN,\"FP\":FP,\"FN\":FN,\"Accuracy\":Classification_Accuracy,\"Error\":Classification_Error,\"Sensitivity\":Sensitivity,\"Specificity\":Specificity,\"False_Positive_Rate\":False_Positive_Rate,\"Precision\":Precision})\n",
    "\n",
    "\n",
    "df = pd.DataFrame(matricsTotal).head()\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "#df.plot.figure(figsize=(50,50))\n",
    "df.plot(kind = 'bar',figsize =(10 ,10),stacked = False)\n",
    "\n",
    "print df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking with 2 class n features Gaussian discreminent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,4) and (1,4) not aligned: 4 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-b741b8d6d2d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mGDAnD2C\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mpredicted_Y_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mtm\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mTestMetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mtm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredicted_Y_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-d5ac5d6a0dd3>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, XD)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m# this is to predict y data  from the give X data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mXD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscriminent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mxvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mXD\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-ae4e81576906>\u001b[0m in \u001b[0;36mdiscriminent\u001b[1;34m(x, a_class, b_class)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdiscriminent\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0msendX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;31m# sending x as a collection of rows vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ma_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msendX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mb_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msendX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32melse\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-ae4e81576906>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x, co_var_mat, mean)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mcovVar\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[1;32mlambda\u001b[0m \u001b[0mX_D\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_D\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcov\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_D\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrowIndex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclassElement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mrowElements\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrowIndex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrowElements\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;32mif\u001b[0m \u001b[0midf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_D\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrowIndex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclassElement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mclassIndex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclassElement\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumberOfClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# this is for 2D calcultaion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mg_multiVariate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mco_var_mat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mco_var_mat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mco_var_mat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# discriminent = lambda x,a_class,b_class : (1,0)[ a_class(x) > b_class(x) ]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1,4) and (1,4) not aligned: 4 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "X_2D = X[:100,:]# this is to take one feature and first 100 data sets\n",
    "Y_2D = Y [:100] # this is to take 1st 100 data sets\n",
    "\n",
    "matricsTotal = []\n",
    "kf = sklearn.cross_validation.KFold(n=len(Y_1D), n_folds=10, shuffle=True,random_state=5)\n",
    "for train_index, test_index in kf:\n",
    "    #print train_index\n",
    "#     X_train, X_test, Y_train, Y_test = train_test_split(X_1D, Y_1D, test_size=0.4, random_state= a)\n",
    "    X_train, X_test =[X_2D[a] for a  in train_index],[X_2D[a] for a  in test_index]\n",
    "    Y_train, Y_test = [Y_2D[a] for a  in train_index],[Y_2D[a] for a  in test_index]\n",
    "#     print X_train\n",
    "#     #     print Y_train\n",
    "#     print \"\\n\\n\"\n",
    "    grad =  GDAnD2C()\n",
    "    grad.fit(X_train,Y_train)\n",
    "    predicted_Y_values = grad.predict(X_test)\n",
    "    tm  = TestMetrics()\n",
    "    tm.setData(Y_test,predicted_Y_values)\n",
    "    TP,TN,FP,FN,Classification_Accuracy,Classification_Error,Sensitivity,Specificity,False_Positive_Rate,Precision =  tm.getMatrics()\n",
    "    matricsTotal.append({\"TP\":TP,\"TN\":TN,\"FP\":FP,\"FN\":FN,\"Accuracy\":Classification_Accuracy,\"Error\":Classification_Error,\"Sensitivity\":Sensitivity,\"Specificity\":Specificity,\"False_Positive_Rate\":False_Positive_Rate,\"Precision\":Precision})\n",
    "\n",
    "\n",
    "df = pd.DataFrame(matricsTotal).head()\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "#df.plot.figure(figsize=(50,50))\n",
    "df.plot(kind = 'bar',figsize =(10 ,10),stacked = False)\n",
    "\n",
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "asdf = np.array([[1,2,4]])\n",
    "print (asdf.T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = [1, 2]\n",
    "asd = np.asarray([list(a)])\n",
    "print asd.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GDAnD3C:\n",
    "    def fit(self,Xdata,Ydata):\n",
    "        self.XData = Xdata\n",
    "        self.YData = Ydata\n",
    "    \n",
    "        #step:2 - find mean \n",
    "        self.mean = mean(Xdata,Ydata)\n",
    "        #print \"mean\",self.mean\n",
    "        \n",
    "        #step:3 - find co_varience\n",
    "        self.co_varience = covVar(Xdata,Ydata)\n",
    "        #print\"co varience matix\",self.co_varience\n",
    "\n",
    "        \n",
    "        ## sending the mean as transposed\n",
    "        self.g0 = partial(g_multiVariate,co_var_mat = self.co_varience[0],mean= (np.asarray([self.mean[0]])).T)\n",
    "        self.g1 = partial(g_multiVariate,co_var_mat = self.co_varience[1],mean= (np.asarray([self.mean[1]])).T)\n",
    "        self.g2 = partial(g_multiVariate,co_var_mat = self.co_varience[2],mean= (np.asarray([self.mean[2]])).T)\n",
    "\n",
    "        #step:5\n",
    "        self.discriminent = partial(discriminent_1,a_class = self.g0, b_class = self.g1, c_class = self.g2)\n",
    "\n",
    "#         self.gs = [partial(g_multiVariate,co_var_mat = self.co_varience[itr],mean= (np.asarray([self.mean[itr]])).T) for itr in range(0,len(Ydata))]\n",
    "#         #step:5\n",
    "#         self.discriminent = partial(discriminent_1,a_class = self.gs[0], b_class = self.gs[1],c_class = self.gs[2])\n",
    "#         #self.discriminent = partial(discriminent,a_class = partial(g,varience = self.varience[0],mean= self.mean[0],alpha= self.alpha[0]), b_class = partial(g,varience = self.varience[1],mean= self.mean[1],alpha= self.alpha[1]))\n",
    "\n",
    "    def predict(self,XD):\n",
    "        return [self.discriminent(xvalue) for xvalue in XD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print [i for i in len([1,2,3,4])]\n",
    "print [a for a in range(0,len([1,2,3,4]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_nD = X[:,:]# this is to take one feature and first 100 data sets\n",
    "Y_nD = Y [:] # this is to take 1st 100 data sets\n",
    "#print np.sum(X_12)/50\n",
    "#print Y_1D.size\n",
    "# print X_1D\n",
    "# print Y_1D\n",
    "# print len(X_1D)\n",
    "# print alpha(X_1D,Y_1D)\n",
    "# print mean(X_1D,Y_1D)\n",
    "# print varience(X_1D,Y_1D)[0]\n",
    "# STEP 1: split X and y into training and testing set\n",
    "matricsTotal = []\n",
    "kf = sklearn.cross_validation.KFold(n=len(Y_nD), n_folds=10, shuffle=True)\n",
    "for train_index, test_index in kf:\n",
    "    #print train_index\n",
    "#     X_train, X_test, Y_train, Y_test = train_test_split(X_1D, Y_1D, test_size=0.4, random_state= a)\n",
    "    X_train, X_test =[X_nD[a] for a  in train_index],[X_nD[a] for a  in test_index]\n",
    "    Y_train, Y_test = [Y_nD[a] for a  in train_index],[Y_nD[a] for a  in test_index]\n",
    "#     print X_train\n",
    "#     #     print Y_train\n",
    "#     print \"\\n\\n\"\n",
    "    grad =  GDAnD3C()\n",
    "    grad.fit(X_train,Y_train)\n",
    "    predicted_Y_values = grad.predict(X_test)\n",
    "    tm  = TestMetrics()\n",
    "    tm.setData(Y_test,predicted_Y_values)\n",
    "    TP,TN,FP,FN,Classification_Accuracy,Classification_Error,Sensitivity,Specificity,False_Positive_Rate,Precision =  tm.getMatrics()\n",
    "    matricsTotal.append({\"TP\":TP,\"TN\":TN,\"FP\":FP,\"FN\":FN,\"Accuracy\":Classification_Accuracy,\"Error\":Classification_Error,\"Sensitivity\":Sensitivity,\"Specificity\":Specificity,\"False_Positive_Rate\":False_Positive_Rate,\"Precision\":Precision})\n",
    "\n",
    "\n",
    "df = pd.DataFrame(matricsTotal).head()\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "#df.plot.figure(figsize=(50,50))\n",
    "df.plot(kind = 'bar',figsize =(10 ,10),stacked = False)\n",
    "\n",
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 4 6 8]]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2,3,4]])\n",
    "b = np.array([[1,2,3,4]])\n",
    "print a + b \n",
    "print a.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
